{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import time\n",
    "from dotenv import load_dotenv\n",
    "from datetime import datetime, timedelta\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StringType, StructType, StructField, TimestampType, DoubleType, ArrayType\n",
    "from pyspark.sql.functions import from_json, udf, col, array_contains, split, count\n",
    "from pyspark.sql import functions as F\n",
    "from pymongo import MongoClient\n",
    "\n",
    "from tools import get_list_of_keys\n",
    "\n",
    "CURRENCIES = get_list_of_keys('symbol')\n",
    "\n",
    "load_dotenv()\n",
    "MONGODB_ATLAS_USER = os.getenv(\"MONGODB_ATLAS_USER\")\n",
    "MONGODB_ATLAS_PASSWORD = os.getenv(\"MONGODB_ATLAS_PASSWORD\")\n",
    "MONGODB_ATLAS_URI = \"mongodb+srv://{}:{}@cluster0.6jprsq1.mongodb.net/\".format(MONGODB_ATLAS_USER, MONGODB_ATLAS_PASSWORD)\n",
    "MONGO_DB_NAME = os.getenv(\"MONGODB_ATLAS_DATABASE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "pymongo_client = MongoClient(MONGODB_ATLAS_URI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n",
      ":: loading settings :: url = jar:file:/usr/local/lib/python3.9/dist-packages/pyspark/jars/ivy-2.4.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n",
      "org.mongodb.spark#mongo-spark-connector_2.12 added as a dependency\n",
      "org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-11d77ce1-0975-4ff0-bc02-6a0451d17377;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.mongodb.spark#mongo-spark-connector_2.12;3.0.1 in central\n",
      "\tfound org.mongodb#mongodb-driver-sync;4.0.5 in central\n",
      "\tfound org.mongodb#bson;4.0.5 in central\n",
      "\tfound org.mongodb#mongodb-driver-core;4.0.5 in central\n",
      "\tfound org.apache.spark#spark-sql-kafka-0-10_2.12;3.0.0 in central\n",
      "\tfound org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.0.0 in central\n",
      "\tfound org.apache.kafka#kafka-clients;2.4.1 in central\n",
      "\tfound com.github.luben#zstd-jni;1.4.4-3 in central\n",
      "\tfound org.lz4#lz4-java;1.7.1 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.7.5 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.30 in central\n",
      "\tfound org.spark-project.spark#unused;1.0.0 in central\n",
      "\tfound org.apache.commons#commons-pool2;2.6.2 in central\n",
      ":: resolution report :: resolve 373ms :: artifacts dl 7ms\n",
      "\t:: modules in use:\n",
      "\tcom.github.luben#zstd-jni;1.4.4-3 from central in [default]\n",
      "\torg.apache.commons#commons-pool2;2.6.2 from central in [default]\n",
      "\torg.apache.kafka#kafka-clients;2.4.1 from central in [default]\n",
      "\torg.apache.spark#spark-sql-kafka-0-10_2.12;3.0.0 from central in [default]\n",
      "\torg.apache.spark#spark-token-provider-kafka-0-10_2.12;3.0.0 from central in [default]\n",
      "\torg.lz4#lz4-java;1.7.1 from central in [default]\n",
      "\torg.mongodb#bson;4.0.5 from central in [default]\n",
      "\torg.mongodb#mongodb-driver-core;4.0.5 from central in [default]\n",
      "\torg.mongodb#mongodb-driver-sync;4.0.5 from central in [default]\n",
      "\torg.mongodb.spark#mongo-spark-connector_2.12;3.0.1 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.30 from central in [default]\n",
      "\torg.spark-project.spark#unused;1.0.0 from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.7.5 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   13  |   0   |   0   |   0   ||   13  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-11d77ce1-0975-4ff0-bc02-6a0451d17377\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 13 already retrieved (0kB/7ms)\n",
      "22/11/16 16:51:20 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession. \\\n",
    "    builder. \\\n",
    "    appName(\"pyspark-notebook\"). \\\n",
    "    master(\"spark://spark-master:7077\"). \\\n",
    "    config(\"spark.executor.memory\", \"2g\"). \\\n",
    "    config(\"spark.mongodb.input.uri\", MONGODB_ATLAS_URI). \\\n",
    "    config(\"spark.mongodb.output.uri\", MONGODB_ATLAS_URI). \\\n",
    "    config(\"spark.jars.packages\", \"org.mongodb.spark:mongo-spark-connector_2.12:3.0.1,org.apache.spark:spark-sql-kafka-0-10_2.12:3.0.0\"). \\\n",
    "    getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/11/16 16:51:24 WARN StreamingQueryManager: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-457d0805-5efa-4a9f-ad05-4920948a841c. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.streaming.StreamingQuery at 0x7f10e86f14c0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_matching_hashtags(tweet: str) -> list:\n",
    "    hashtags = re.findall('([#][a-zA-Z]+)', str(tweet))\n",
    "    return [item for item in CURRENCIES if '#{}'.format(item) in (tag.lower() for tag in hashtags)]\n",
    "\n",
    "tweet_stream_df = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"broker:29092\") \\\n",
    "    .option(\"subscribe\", \"tweeting\") \\\n",
    "    .load()\n",
    "tweet_schema = StructType([StructField(\"text\", StringType(), True), StructField('date', TimestampType(), True)])\n",
    "tweets_values = tweet_stream_df.select(from_json(tweet_stream_df.value.cast(\"string\"), tweet_schema).alias(\"tweet\"))\n",
    "\n",
    "df1 = tweets_values.select(\"tweet.*\")\n",
    "clean_tweets = F.udf(get_matching_hashtags, StringType())\n",
    "raw_tweets = df1.withColumn('cryptos', clean_tweets(col(\"text\")))\n",
    "\n",
    "def write_row_in_tweet_mongo(df, id):\n",
    "    df.write.format(\"mongo\").mode(\"append\").option(\"uri\", MONGODB_ATLAS_URI + \"development.tweets\" + \"?retryWrites=true&w=majority\").save()\n",
    "    pass\n",
    "\n",
    "raw_tweets \\\n",
    "    .writeStream \\\n",
    "    .foreachBatch(write_row_in_tweet_mongo) \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "crypto_stream_df = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"broker:29092\") \\\n",
    "    .option(\"subscribe\", \"crypto\") \\\n",
    "    .load()\n",
    "crypto_schema = StructType([StructField(\"name\", StringType(), True), StructField(\"symbol\", StringType(), True), StructField(\"value\", DoubleType(), True), StructField('date', TimestampType(), True)])\n",
    "crypto_values = crypto_stream_df.select(from_json(crypto_stream_df.value.cast(\"string\"), crypto_schema).alias(\"crypto\"))\n",
    "df_crypto = crypto_values.select(\"crypto.*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/11/16 16:51:24 WARN StreamingQueryManager: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-94609a2f-5058-4aac-94d8-a2330b8cb125. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.streaming.StreamingQuery at 0x7f10e8686ac0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def write_row_in_crypto_mongo(df, id):\n",
    "    df.write.format(\"mongo\").mode(\"append\").option(\"uri\", MONGODB_ATLAS_URI + \"development.cryptos\" + \"?retryWrites=true&w=majority\").save()\n",
    "    pass\n",
    "\n",
    "df_crypto \\\n",
    "    .writeStream \\\n",
    "    .foreachBatch(write_row_in_crypto_mongo) \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "def generate_result(df_to_treat, db, crypto):\n",
    "    data = {\n",
    "        \"nbr_tweets\": df_to_treat.count(),\n",
    "        \"crypto_name\": crypto,\n",
    "        \"datetime\": datetime.now() + timedelta(hours=1)\n",
    "    }\n",
    "    db['results'].insert_one(data)\n",
    "\n",
    "@udf(returnType = ArrayType(StringType()))\n",
    "def clean_crypto_array(value):\n",
    "    return value.strip('[]').split(',')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = pymongo_client[MONGO_DB_NAME]\n",
    "\n",
    "time_between_treatment = 900\n",
    "while True:\n",
    "    db[\"tweets\"].drop()\n",
    "    time.sleep(time_between_treatment)\n",
    "    tweets_df = spark.read.format(\"mongo\").option(\"uri\", MONGODB_ATLAS_URI + \"development.tweets\").load()\n",
    "    df = tweets_df.withColumn('crypto_array', clean_crypto_array(\"cryptos\")).drop(\"cryptos\")\n",
    "    for crypto in CURRENCIES:\n",
    "        generate_result(df.where(array_contains(df['crypto_array'], crypto)), db, crypto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
